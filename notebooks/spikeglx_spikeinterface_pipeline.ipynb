{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface pipeline for Brody Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "import spikeextractors as se\n",
    "import spiketoolkit as st\n",
    "import spikesorters as ss\n",
    "import spikecomparison as sc\n",
    "import spikewidgets as sw\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) Load AP recordings, LF recordings and TTL signals\n",
    "\n",
    "### ToDo: add all data types and adjust path options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"E:/Brody\")\n",
    "raw_data_path = base_path / \"Neuropixels_Feldman\" / \"210209\" / \"SpikeGLX\"\n",
    "session_name = \"LR_210209_g0\"\n",
    "# session_name = \"LR_210209_g1\"\n",
    "# session_name = \"LR_210209_2_g0\"\n",
    "# session_name = \"LR_210209_2_g1\"\n",
    "ap_bin_path = raw_data_path / session_name / f\"{session_name}_imec0\" / f\"{session_name}_g0_t0.imec0.ap.bin\"\n",
    "lf_bin_path = raw_data_file.parent / raw_data_file.name.replace(\"ap\", \"lf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make spikeinterface folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_folder = raw_data_path / session_name\n",
    "spikeinterface_folder = recording_folder / \"spikeinterface\"\n",
    "spikeinterface_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) stub recording for fast testing; set to False for running processing pipeline on entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stub_test = True\n",
    "nsec_stub = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_ap = se.SpikeGLXRecordingExtractor(ap_bin_path)\n",
    "recording_lf = se.SpikeGLXRecordingExtractor(lf_bin_path)\n",
    "\n",
    "if stub_test:\n",
    "    recording_ap = se.SubRecordingExtractor(recording_ap, end_frame=int(nsec_stub*recording_ap.get_sampling_frequency()))\n",
    "    recording_lf = se.SubRecordingExtractor(recording_lf, end_frame=int(nsec_stub*recording_lf.get_sampling_frequency()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sampling frequency AP: {recording_ap.get_sampling_frequency()}\")\n",
    "print(f\"Sampling frequency LF: {recording_lf.get_sampling_frequency()}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts_ap = sw.plot_timeseries(recording_ap, channel_ids=recording_ap.get_channel_ids()[::4], trange=[100, 105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ts_lf = sw.plot_timeseries(recording_lf, channel_ids=recording_lf.get_channel_ids()[::4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_cmr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_cmr:\n",
    "    recording_processed = st.preprocessing.common_reference(recording_ap_sync)\n",
    "else:\n",
    "    recording_processed = recording_ap_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = recording_processed.get_num_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates, amps = st.postprocessing.compute_channel_spiking_activity(\n",
    "    recording_processed,\n",
    "    n_jobs=16,\n",
    "    chunk_mb=4000,\n",
    "    start_frame=10*30000,\n",
    "    end_frame=20*30000, \n",
    "    detect_threshold=8,\n",
    "    recompute_info=True, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2)\n",
    "sw.plot_activity_map(recording_processed, activity=\"rate\", colorbar=True, ax=axs[0])\n",
    "sw.plot_activity_map(recording_processed, activity=\"amplitude\", colorbar=True, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run spike sorters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter_list = [\n",
    "    \"ironclust\",\n",
    "    # \"tridesclous\",\n",
    "    #'spykingcircus',\n",
    "    # 'herdingspikes',\n",
    "    #'kilosort2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ironclust params description:\n",
      "{'adjacency_radius': 'Use -1 to include all channels in every neighborhood',\n",
      " 'adjacency_radius_out': 'Use -1 to include all channels in every neighborhood',\n",
      " 'batch_sec_drift': 'Batch duration in seconds. clustering time duration',\n",
      " 'chunk_mb': 'Chunk size in Mb for saving to binary format (default 500Mb)',\n",
      " 'clip_post': 'Post-peak clip duration in ms',\n",
      " 'clip_pre': 'Pre-peak clip duration in ms',\n",
      " 'common_ref_type': 'Common reference type: none, mean, median, trimmean',\n",
      " 'delta_cut': 'Cluster detection threshold (delta-cutoff)',\n",
      " 'detect_sign': 'Use -1 (negative), 1 (positive) or 0 (both) depending on the '\n",
      "                'sign of the spikes in the recording',\n",
      " 'detect_threshold': 'detection threshold',\n",
      " 'fGpu': 'Use GPU if True',\n",
      " 'fParfor': 'Parfor loop',\n",
      " 'feature_type': 'gpca, pca, vpp, vmin, vminmax, cov, energy, xcov',\n",
      " 'fft_thresh': 'FFT-based noise peak threshold',\n",
      " 'fft_thresh_low': 'FFT-based noise peak lower threshold (set to 0 to disable '\n",
      "                   'dual thresholding scheme)',\n",
      " 'filter': 'Enable or disable filter',\n",
      " 'filter_detect_type': 'Filter type for detection: none, bandpass, wiener, '\n",
      "                       'fftdiff, ndiff',\n",
      " 'filter_type': 'Filter type: none, bandpass, wiener, fftdiff, ndiff',\n",
      " 'freq_max': 'Low-pass filter cutoff frequency',\n",
      " 'freq_min': 'High-pass filter cutoff frequency',\n",
      " 'knn': 'K nearest neighbors',\n",
      " 'merge_overlap_thresh': 'Knn-overlap merge threshold',\n",
      " 'merge_thresh': 'Threshold for automated merging',\n",
      " 'merge_thresh_cc': 'Cross-correlogram merging threshold, set to 1 to disable',\n",
      " 'min_count': 'Minimum cluster size',\n",
      " 'nRepeat_merge': 'Number of repeats for merge',\n",
      " 'nSites_whiten': 'Number of adjacent channels to whiten',\n",
      " 'n_jobs_bin': 'Number of jobs for saving to binary format (Default 1)',\n",
      " 'pc_per_chan': 'Number of principal components per channel',\n",
      " 'post_merge_mode': 'Post merge mode',\n",
      " 'prm_template_name': '.prm template file name',\n",
      " 'sort_mode': 'Sort mode',\n",
      " 'step_sec_drift': 'Compute anatomical similarity every n sec',\n",
      " 'whiten': 'Whether to do channel whitening as part of preprocessing'}\n",
      "Default params:\n",
      "{'adjacency_radius': 50,\n",
      " 'adjacency_radius_out': 100,\n",
      " 'batch_sec_drift': 300,\n",
      " 'chunk_mb': 500,\n",
      " 'clip_post': 0.75,\n",
      " 'clip_pre': 0.25,\n",
      " 'common_ref_type': 'trimmean',\n",
      " 'delta_cut': 1,\n",
      " 'detect_sign': -1,\n",
      " 'detect_threshold': 3.5,\n",
      " 'fGpu': True,\n",
      " 'fParfor': False,\n",
      " 'feature_type': 'gpca',\n",
      " 'fft_thresh': 8,\n",
      " 'fft_thresh_low': 0,\n",
      " 'filter': True,\n",
      " 'filter_detect_type': 'none',\n",
      " 'filter_type': 'bandpass',\n",
      " 'freq_max': 8000,\n",
      " 'freq_min': 300,\n",
      " 'knn': 30,\n",
      " 'merge_overlap_thresh': 0.95,\n",
      " 'merge_thresh': 0.985,\n",
      " 'merge_thresh_cc': 1,\n",
      " 'min_count': 30,\n",
      " 'nRepeat_merge': 3,\n",
      " 'nSites_whiten': 16,\n",
      " 'n_jobs_bin': 1,\n",
      " 'pc_per_chan': 9,\n",
      " 'post_merge_mode': 1,\n",
      " 'prm_template_name': '',\n",
      " 'sort_mode': 1,\n",
      " 'step_sec_drift': 20,\n",
      " 'whiten': False}\n"
     ]
    }
   ],
   "source": [
    "# Inspect sorter-specific parameters and defaults\n",
    "for sorter in sorter_list:\n",
    "    print(f\"{sorter} params description:\")\n",
    "    pprint(ss.get_params_description(sorter))\n",
    "    print(\"Default params:\")\n",
    "    pprint(ss.get_default_params(sorter))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-specific parameters\n",
    "sorter_params = dict(\n",
    "    #kilosort2=dict(car=False, n_jobs_bin=12, chunk_mb=4000),\n",
    "    #ironclust=dict(filter=True),\n",
    "    tridesclous=dict(n_jobs_bin=12, chunk_mb=4000),\n",
    "    spykingcircus=dict(filter=True, num_workers=16),\n",
    "    herdingspikes=dict(filter=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vary_sorter_params = dict(\n",
    "    # Sorter name is first key\n",
    "    ironclust=dict(\n",
    "        # Parameter name is second key\n",
    "        adjacency_radius = dict(\n",
    "            # Third keys define the tuning intervals (min:by:max)\n",
    "            min=25,\n",
    "            max=75,\n",
    "            by=5\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_outputs = dict()\n",
    "\n",
    "for sorter_name in vary_sorter_params:\n",
    "    sorting_outputs.update(\n",
    "        {\n",
    "            sorter_name: ss.run_sorters(\n",
    "                sorter_list=sorter_name, \n",
    "                recording_dict_or_list=dict(rec0=recording_processed),\n",
    "                working_folder=spikeinterface_folder / \"working1\",\n",
    "                mode=\"keep\", # change to \"keep\" to avoid repeating the spike sorting\n",
    "                sorter_params=sorter_params,\n",
    "                verbose=True,\n",
    "                run_sorter_kwargs=dict(raise_error=False)\n",
    "            )\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    print(f\"{sorter} found {len(sorting.get_unit_ids())} units\")\n",
    "    \n",
    "    # tridesclous sometimes has empty clusters\n",
    "    active_units = []\n",
    "    for u in sorting.get_unit_ids():\n",
    "        if len(sorting.get_unit_spike_train(u)) > 0:\n",
    "            active_units.append(u)\n",
    "    \n",
    "    if len(active_units) < len(sorting.get_unit_ids()):\n",
    "        sorting_outputs[result_name] = se.SubSortingExtractor(sorting, unit_ids=active_units)\n",
    "        print(f\"{sorter} found {len(active_units)} units after removing empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorting_tdc = sorting_outputs[('rec0', 'tridesclous')]\n",
    "sw.plot_unit_templates(recording_processed, sorting_tdc, unit_ids=[11, 21], radius=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Post-processing: extract waveforms, templates, quality metrics, extracellular features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set postprocessing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing params\n",
    "postprocessing_params = st.postprocessing.get_postprocessing_params()\n",
    "pprint(postprocessing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (optional) change parameters\n",
    "postprocessing_params['max_spikes_per_unit'] = 1000  # with None, all waveforms are extracted\n",
    "postprocessing_params['n_jobs'] = 16  # n jobs\n",
    "postprocessing_params['chunk_mb'] = 4000  # max RAM usage in Mb\n",
    "postprocessing_params['verbose'] = True  # max RAM usage in Mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set quality metric list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality metrics\n",
    "qc_list = st.validation.get_quality_metrics_list()\n",
    "print(f\"Available quality metrics: {qc_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (optional) define subset of qc\n",
    "qc_list = ['snr', 'isi_violation', 'firing_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set extracellular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracellular features\n",
    "ec_list = st.postprocessing.get_template_features_list()\n",
    "print(f\"Available EC features: {ec_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (optional) define subset of ec\n",
    "ec_list = ['peak_to_valley', 'halfwidth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Postprocess all sorting outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    print(f\"Postprocessing recording {rec_name} sorted with {sorter}\")\n",
    "    tmp_folder = spikeinterface_folder / 'tmp' / sorter\n",
    "    tmp_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # set local tmp folder\n",
    "    sorting.set_tmp_folder(tmp_folder)\n",
    "    \n",
    "    # compute waveforms\n",
    "    waveforms = st.postprocessing.get_unit_waveforms(recording_processed, sorting, **postprocessing_params)\n",
    "    \n",
    "    # compute templates\n",
    "    templates = st.postprocessing.get_unit_templates(recording_processed, sorting, **postprocessing_params)\n",
    "    \n",
    "    # comput EC features\n",
    "    ec = st.postprocessing.compute_unit_template_features(\n",
    "        recording_processed,\n",
    "        sorting,\n",
    "        feature_names=ec_list,\n",
    "        as_dataframe=True\n",
    "    )\n",
    "\n",
    "    # compute QCs\n",
    "    qc = st.validation.compute_quality_metrics(\n",
    "        sorting,\n",
    "        recording=recording_processed, \n",
    "        metric_names=qc_list,\n",
    "        as_dataframe=True\n",
    "    )\n",
    "    \n",
    "    # export to phy\n",
    "    if sorter == \"kilosort2\":\n",
    "        phy_folder = spikeinterface_folder / 'phy' / sorter\n",
    "        phy_folder.mkdir(parents=True, exist_ok=True)\n",
    "        st.postprocessing.export_to_phy(recording_processed, sorting, phy_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run phy and load curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!phy template-gui spikeinterface/phy/kilosort2/params.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_manual_curated = se.PhySortingExtractor(phy_folder, exclude_cluster_groups=['noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Kilosort2 found {len(sorting_manual_curated.get_unit_ids())} units after manual curation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5) Ensemble spike sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if len(sorting_outputs) > 1:\n",
    "    # retrieve sortings and sorter names\n",
    "    sorting_list = []\n",
    "    sorter_names_comp = []\n",
    "    for result_name, sorting in sorting_outputs.items():\n",
    "        rec_name, sorter = result_name\n",
    "        sorting_list.append(sorting)\n",
    "        sorter_names_comp.append(sorter)\n",
    "        \n",
    "    # run multisorting comparison\n",
    "    mcmp = sc.compare_multiple_sorters(sorting_list=sorting_list, name_list=sorter_names_comp)\n",
    "    \n",
    "    # plot agreement results\n",
    "    w_agr = sw.plot_multicomp_agreement(mcmp)\n",
    "    \n",
    "    # extract ensamble sorting\n",
    "    sorting_ensemble = mcmp.get_agreement_sorting(minimum_agreement_count=2)\n",
    "    \n",
    "    print(f\"Ensemble sorting among {sorter_list} found: {len(sorting_ensemble.get_unit_ids())} units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.plot_rasters(sorting_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6) Automatic curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define curators and thresholds\n",
    "isi_violation_threshold = 0.5\n",
    "snr_threshold = 5\n",
    "firing_rate_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorting_auto_curated = []\n",
    "sorter_names_curation = []\n",
    "for result_name, sorting in sorting_outputs.items():\n",
    "    rec_name, sorter = result_name\n",
    "    sorter_names_curation.append(sorter)\n",
    "    \n",
    "    # firing rate threshold\n",
    "    sorting_curated = st.curation.threshold_firing_rates(\n",
    "        sorting,\n",
    "        duration_in_frames=num_frames,\n",
    "        threshold=firing_rate_threshold, \n",
    "        threshold_sign='less'\n",
    "    )\n",
    "    \n",
    "    # isi violation threshold\n",
    "    sorting_curated = st.curation.threshold_isi_violations(\n",
    "        sorting_curated,\n",
    "        duration_in_frames=num_frames,\n",
    "        threshold=isi_violation_threshold, \n",
    "        threshold_sign='greater'\n",
    "    )\n",
    "    \n",
    "    # isi violation threshold\n",
    "    sorting_curated = st.curation.threshold_snrs(\n",
    "        sorting_curated,\n",
    "        recording=recording_processed,\n",
    "        threshold=snr_threshold, \n",
    "        threshold_sign='less'\n",
    "    )\n",
    "    sorting_auto_curated.append(sorting_curated)\n",
    "    print(f\"{sorter} found {len(sorting_curated.get_unit_ids())} units after auto curation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 7) Save to NWB; writes only the spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the NWBFile containing behavioral or full recording data\n",
    "nwbfile_path = base_data_path / f\"Brody_PoissonClicks_{session_name}.nwb\"\n",
    "\n",
    "# Choose the sorting extractor from the notebook environment you would like to write to NWB\n",
    "chosen_sorting_extractor = sorting_outputs[('rec0', 'ironclust')]\n",
    "\n",
    "se.NwbSortingExtractor.write_sorting(\n",
    "    sorting=chosen_sorting_extractor,\n",
    "    save_path=nwbfile_path,\n",
    "    overwrite=False  # this appends the file. True would write a new file\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
